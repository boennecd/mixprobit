---
output:
  md_document:
    variant: markdown_github
    pandoc_args: --webtex=https://latex.codecogs.com/svg.latex?
bibliography: ../README.bib
---

# Salamander Data Set

```{r setup, echo = FALSE, cache=FALSE}
local({
  if(!grepl("examples$", getwd()))
    setwd("examples")
})

knitr::opts_chunk$set(
  collapse = TRUE, comment = "#>", fig.path = "fig-salamander/", dpi = 124, 
  error = FALSE, cache.path = "cache-salamander/")
options(digits = 4)
```

Load the data

```{r load_dat}
sala <- read.csv("salam.csv", header = TRUE, sep = "")
head(sala)

sala <- within(sala, {
  female <- as.factor(female)
  male   <- as.factor(male) 
})
```

Assign cluster (TODO: we must be able to this in a smart way...)

```{r asg_cluster}
sala$cl <- with(sala, {
  cl <- rep(NA_integer_, length(female))
  
  grp <- 0L
  repeat {
    # take first NA
    grp <- grp + 1L
    cur <- which(is.na(cl))
    if(length(cur) == 0L)
      # not any left. Thus return
      break
    
    new_members <- cur[1L]
    repeat {
      cl[new_members] <- grp
      male_in_grp    <- male  [new_members]
      females_in_grp <- female[new_members]
      
      # find mates of the new members
      new_members <- which(
        ((male %in% male_in_grp) | (female %in% females_in_grp)) & 
          is.na(cl))
      if(length(new_members) == 0L)
        break
    }
  }
  
  cl
})

# turns out that the data was already sorted...
stopifnot(!is.unsorted(sala$cl))
with(sala, {
  female <- as.integer(female)
  male   <- as.integer(male)
  
  p <- length(unique(female))
  stopifnot(length(unique(male)) == p)
  
  X <- matrix(0L, p, p)
  for(i in 1:length(female))
    X[female[i], male[i]] <- 1L
  
  # show plot of males and females that mate
  image(X, xlab = "female", ylab = "male")
})

# get summary stats for clusters
local({
  sum_dat <- sapply(split(sala, sala$cl), function(x){
    p <- length(unique(x$female))
    q <- length(unique(x$male))
    
    c(`# observations (n)` = NROW(x), `# random effects (p)` = p + q)
  })
  dimnames(sum_dat) <- structure(dimnames(sum_dat), 
                                 names = c("Stat", "Cluster"))
  t(sum_dat)
})
```

Assign the formulas that we need

```{r asg_forms}
# Elements: 
#   X: fixed effect formula.
#   Z: random effect formula.
frm <- list(X = ~ wsm * wsf, Z = ~ female + male - 1)
```

## Fit Model Without Random Effects

```{r no_rng}
summary(glm_fit <- glm(update(frm$X, y ~ .), binomial("probit"), sala))

logLik(glm_fit)
```

## lmec

Will not work as the `varstruct` argument can only be `"unstructured"` or 
`"diagonal"`.

```{r lmec_fit, eval = FALSE, echo = FALSE}
library(lmec)

lmec_analysis <- within(list(), {
  n <- NROW(sala)
  y_dum <- ifelse(sala$y, 1, -1)
  Z <- model.matrix(frm$Z, sala) * y_dum
  X <- model.matrix(frm$X, sala) * y_dum
  
  fit <- lmec(yL = rep(0, n), cens = rep(TRUE, n), X = X, Z = Z, 
              cluster = rep(1L, n), varstruct = "diagonal")
})
```

## Stan

Here is the Stan file we use.

```{stan output.var = "dummy", code = readLines("salamander.stan"), eval = FALSE}
```

Next, we fit the model and show the model estimates.

```{r load_stan, message = FALSE}
library(rstan)
options(mc.cores = parallel::detectCores(logical = FALSE))
rstan_options(auto_write = TRUE)
```

```{r fit_Stan, cache = 1}
stan_fit <- within(list(), {
  sala <- as.list(sala)
  sala <- within(sala, {
    J <- length(y)
    K <- length(unique(female))
    female <- as.integer(female)
    male   <- as.integer(male)
  })
  stopifnot(sala$K == length(unique(sala$male)))
  
  fit <- stan(
    file = "salamander.stan", # Stan program
    data = sala,              # named list of data
    chains = 4L,              # number of Markov chains
    warmup = 10000L,          # number of warmup iterations per chain
    iter = 20000L,            # total number of iterations per chain
    cores = 2L,               # number of cores (could use one per chain)
    refresh = 0L,             # no progress shown
    seed = 91154163L)
  
  print(fit, pars = c("beta", "sigma"))
})
```

## glmer (Laplace Approximation)

We only use the Laplace approximation from `glmer` as adaptive 
Gaussâ€“Hermite quadrature is not available with crossed random effects (and
would take forever with this data set).

```{r load_lme4, message = FALSE}
library(lme4)
```

```{r fit_glmer, cache = 1}
glmer_fit <- within(list(), {
  frm_use <- y ~ wsm * wsf + (1 | female) + (1 | male)
  fit_laplace <- glmer(frm_use, sala, binomial("probit"))

  msg <- "Laplace fit"
  cat(msg, "\n", rep("-", nchar(msg)), "\n", sep ="")
  print(fit_laplace)
  
  # AGHQ does not work...
  try(fit_aghq <- glmer(frm_use, sala, binomial("probit"), nAGQ = 5L))
})
```

## CDF Approximation and Genz & Monahan Approximation

Fit models with the CDF approximation like in @Pawitan04 and the method by 
@Genz99.

```{r load_mixprobit}
library(mixprobit)
library(parallel)
```

<!-- knitr::opts_knit$set(output.dir = ".") -->
<!-- knitr::load_cache("cdf_arpx", path = "cache-salamander/") -->

```{r cdf_arpx, cache = 1, warning = FALSE}
mix_prob_fit <- within(list(), {
  # setup cluster
  n_threads <- 6L
  cl <- makeCluster(n_threads)
  on.exit(stopCluster(cl))
  
  # run fit to get starting values
  pre_fit <- glm(update(frm$X, y ~ .), family = binomial("probit"), sala)
  X_terms <- delete.response(terms(pre_fit))
  
  # get data for each cluster
  dat <- lapply(split(sala, sala$cl), function(cl_dat)
    within(list(), {
      cl_dat$female <- droplevels(cl_dat$female)
      cl_dat$male   <- droplevels(cl_dat$male)
      
      y <- cl_dat$y 
      Z <- t(model.matrix(frm$Z, cl_dat))
      X <-   model.matrix(X_terms, cl_dat)
      
      p <- NROW(Z)
      is_male <- which(grepl("^male", rownames(Z)))
      var_idx <- as.integer(grepl("^male", rownames(Z)))
  }))
  
  # starting values
  beta <- pre_fit$coefficients
  fnscale <- abs(c(logLik(pre_fit)))
  q <- length(beta)
  par <- c(beta, log(c(.1, .1)))
  
  # negative log-likelihood function
  ll_func <- function(par, seed = 1L, maxpts = 100000L, abseps = -1, 
                      releps = 1e-2, meth){
    if(!is.null(seed))
      set.seed(seed)
    clusterSetRNGStream(cl)
    beta <-          head(par,  q)
    vars  <- exp(2 * tail(par, -q))
    clusterExport(cl, c("beta", "vars", "maxpts", "abseps", "releps"), 
                  environment())
    
    ll_terms <- parSapply(cl, dat, function(cl_dat){
      with(cl_dat, {
        eta <- drop(X %*% beta)
        Sigma <- diag(nrow(Z))
        
        diag(Sigma)[-is_male] <- vars[1L]
        diag(Sigma)[ is_male] <- vars[2L]
        
        out <- try(meth(
            y = y, eta = eta, Sigma = Sigma, Z = Z, maxpts = maxpts, 
            abseps = abseps, releps = releps))
        
        if(inherits(out, "try-error"))
          return(c(ll = NA_real_, err = NA_real_, inform = 99L))
        
        c(ll = log(c(out)), err = attr(out, "error"), 
          inform = attr(out, "inform"))
      })
    })
    
    inform <- ll_terms["inform", ]
    if(any(inform > 0))
      warning(paste(
        "Got these inform values: ", 
        paste0(unique(inform), collapse = ", ")))
    
    -sum(ll_terms["ll", ])
  }
  
  # C++ version
  cpp_ptr     <- mixprobit:::aprx_binary_mix_cdf_get_ptr(
    data = dat, n_threads = n_threads)
  cpp_ptr_grad <- mixprobit:::aprx_binary_mix_cdf_get_ptr(
    data = dat, n_threads = n_threads, gradient = TRUE)
  
  ll_cpp <- function(par, seed = 1L, maxpts = 100000L, abseps = -1, 
                     releps = 1e-2, gradient = FALSE){
    if(!is.null(seed))
      set.seed(seed)
    
    beta    <- head(par,  q)
    log_sds <- tail(par, -q)
    
    out <- mixprobit:::aprx_binary_mix_cdf_eval(
      ptr = if(gradient) cpp_ptr_grad else cpp_ptr, beta = beta, 
      log_sds = log_sds, maxpts = maxpts, abseps = abseps, releps = releps)
    
    if(gradient)
      -out[-1L] else -out
  }
  ll_cpp_grad <- ll_cpp
  formals(ll_cpp_grad)$gradient <- TRUE
  
  # use the methods to find the optimal parameters
  take_time <- function(expr){
    cat("Running:", sep = "\n",
        paste0("  ", deparse(substitute(expr)), collapse = "\n"), "")
    
    ti <- eval(bquote(system.time(out <- .(substitute(expr)))), 
               parent.frame())
    cat("\n")
    stopifnot(is.list(out) && is.null(out$time))
    out$time <- ti
    out$used_gr <- !is.null(substitute(expr)$gr)
    out
  }
  
  # set formals on optim
  opt_use <- optim
  formals(opt_use)[c("method", "control")] <- list(
    "BFGS", list(trace = 3L, fnscale = fnscale))

  # TODO: move this to a test of gradients to a unit test
  local({
    par <- par + rnorm(par, sd = .1)
    tol <- .Machine$double.eps^(1/3)
    cpp_grad <- drop(
      ll_cpp_grad(par, maxpts = 250000L, releps = tol))
    num_grad <- drop(numDeriv::jacobian(
      function(x) ll_cpp(x, maxpts = 250000L, releps = 1e-4), par, 
      method.args = list(eps = tol)))
    if(!isTRUE(
      ae_res <- all.equal(cpp_grad, num_grad, tolerance = tol^(1/2)))){
        msg <- c(sprintf("%s returned:", sQuote("all.equal")), 
                 paste0("  ", ae_res))
        bx <- paste0(rep("=", max(nchar(msg))), collapse = "")
        cat(bx, msg, bx, "", sep = "\n")
      } else
        cat("Gradient test passed\n")
  })
  
  # first make a few quick fits with a low error or number of samples
  fit_CDF_cpp_fast <- take_time(opt_use(
    par, ll_cpp , maxpts = 5000L, releps = .1, gr = ll_cpp_grad))
  
  GM_meth <- mixprobit:::aprx_binary_mix
  formals(GM_meth)$is_adaptive <- TRUE 
  fit_Genz_Monahan_fast <- take_time(opt_use(
    par, ll_func, maxpts = 1000L, releps = .1,
    meth = GM_meth))
  
  # then use a lower error or more samples starting from the previous 
  # estimate
  eps_use <- 1e-4
  cdf_par <- fit_CDF_cpp_fast$par
  fit_CDF_cpp <- take_time(opt_use(
    cdf_par, ll_cpp , maxpts = 100000L, releps = eps_use, gr = ll_cpp_grad))
  fit_CDF_cpp_wo_grad <- take_time(opt_use(
    cdf_par, ll_cpp , maxpts = 100000L, releps = eps_use))
  fit_CDF <- take_time(opt_use(
    cdf_par, ll_func, maxpts = 100000L, releps = eps_use,
    meth = mixprobit:::aprx_binary_mix_cdf))

  gmo_start <- fit_Genz_Monahan_fast$par
  fit_Genz_Monahan <-  take_time(opt_use(
    gmo_start, ll_func, maxpts = 10000L, releps = eps_use,
    meth = GM_meth))
  
  # add q to output
  fit_CDF_cpp_fast$q <- fit_Genz_Monahan_fast$q <- fit_CDF_cpp$q <- 
    fit_CDF$q <- fit_Genz_Monahan$q <- fit_CDF_cpp_wo_grad$q <- q
})
```

Show the estimates of the methods. The `_cpp` function differs by using an 
almost purely C++ implementation which supports computation in parallel.

```{r show_cdf_aprx}
local({
  show_res <- function(fit){
    nam <- deparse(substitute(fit))
    cat("\n", nam, "\n", rep("-", nchar(nam)), "\n", sep = "")
    
    cat("\nFixed effects\n")
    q <- fit$q
    print(head(fit$par,  q))
    
    cat("\nRandom effect standard deviations")
    print(exp(tail(fit$par, -q)))
    
    fit_time <- fit$time["elapsed"]
    used_gr <- fit$used_gr
    fit_per_eval <- if(used_gr)
      fit_time / fit$counts["function"] else 
        fit_time / (fit$counts["function"] + fit$counts["gradient"] * 2L * 
                      length(fit$par))
    fit_per_grad <- if(used_gr)
      fit_time / fit$counts["gradient"] else NA_real_
      
    cat(sprintf("\nLog-likelihood estimate %.2f\nComputation time %.2f/%.2f (seconds total/per function evaluation)\n", 
                -fit$value, fit_time, fit_per_eval))
    cat("The latter time is not comparable for methods that do not use numerical derivatives\n")
  }
  
  with(mix_prob_fit, {
    show_res(fit_CDF_cpp_fast)
    show_res(fit_Genz_Monahan_fast)
    show_res(fit_CDF)
    show_res(fit_CDF_cpp)
    show_res(fit_CDF_cpp_wo_grad)
    show_res(fit_Genz_Monahan)
  })
})
```

I am not sure but I suspect that the CDF approximation is more precise. 

## Small Clusters

We artificially increase the number of clusters by removing mating pairs to 
grasp the effect on the computation time. First, we remove the pairs. 

```{r make_salamander_small}
sala <- local({
  new_dat <- lapply(split(sala, sala$cl), function(cl_dat){
    male   <- as.integer(cl_dat$male)
    female <- as.integer(cl_dat$female)
    
    # the data is ordered such that the vsm == 0 is first. Thus, we re-order
    # the data
    stopifnot(!is.unsorted(tapply(cl_dat$wsm, male  , unique)), 
              !is.unsorted(tapply(cl_dat$wsf, female, unique)), 
              max(female) == max(male), 
              min(female) == min(male))
    
    idx_vals <- min(female):max(female)
    new_idx <- rep(NA_integer_, length(idx_vals))
    
    is_odd <- (idx_vals %% 2L) == 1L
    res <- sum(is_odd)
    new_idx[ is_odd] <- idx_vals[1:res]
    new_idx[!is_odd] <- idx_vals[setdiff(seq_along(idx_vals), 1:res)]
    
    female <- idx_vals[match(female, new_idx)]
    male   <- idx_vals[match(male  , new_idx)]
    
    # from groups
    denom <- as.integer(ceiling(length(idx_vals) / 2L))
    keep <- (female %/% denom) == (male %/% denom)
    
    cl_dat$male   <- male
    cl_dat$female <- female
    cl_dat <- cl_dat[keep, ]
    cl_dat <- cl_dat[order(cl_dat$female), ]
    cl_dat
  })
  
  out <- do.call(rbind, new_dat)
  out$female <- as.factor(out$female)
  out$male   <- as.factor(out$male)
  out
})
```

```{r again_asg_cluster, ref.label = "asg_cluster"}
```

Then we re-run the estimation. 

```{r again_cdf_arpx, cache = 1, warning = FALSE, ref.label = "cdf_arpx"}
```

```{r show_cdf_aprx}
```

## References
